{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.utils import to_categorical,np_utils\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/\"\n",
    "num_classes = 4\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data from the pickle files\n",
    "with open(path + 'train_image.pkl', 'rb') as f:\n",
    "    X = np.array(pickle.load(f))\n",
    "\n",
    "with open(path + 'train_label.pkl', 'rb') as f:\n",
    "    y = np.array(pickle.load(f))\n",
    "\n",
    "with open(path + 'test_image.pkl', 'rb') as f:\n",
    "    X_submit = np.array(pickle.load(f))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "X = X / 255\n",
    "X_submit = X_submit / 255\n",
    "\n",
    "#reshaping the flattened images to images of dimension (28,28,1)\n",
    "X = np.reshape(X,(len(X),28,28,1))\n",
    "X_test = np.reshape(X_submit,(len(X_submit),28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 2 3 6] \n",
      "Frequency of each label [2000 2000 2000 2000]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Unique labels:\",unique_elements,\"\\nFrequency of each label\",counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining new labels so that the labels are successive integers\n",
    "label_mapping = {0:0,2:1,3:2,6:3}\n",
    "\n",
    "for x in label_mapping.keys():\n",
    "    y = np.where(y==x, label_mapping[x], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] [2000 2000 2000 2000]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(unique_elements,counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Getting list of all four classes\n",
    "type0 = np.where(y==0)[0][0]\n",
    "type1 = np.where(y==1)[0][0]\n",
    "type2 = np.where(y==2)[0][0]\n",
    "type3 = np.where(y==3)[0][0]\n",
    "\n",
    "#Displaying one sample of each class\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.reshape(X[type0],(28,28)), cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.reshape(X[type1],(28,28)), cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(np.reshape(X[type2],(28,28)), cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(np.reshape(X[type3],(28,28)), cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#converting to one hot encodings\n",
    "y = to_categorical(np.array(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5120, 28, 28, 1), (5120, 4))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.16078431],\n",
       "        [0.7372549 ],\n",
       "        [0.40392157],\n",
       "        [0.21176471],\n",
       "        [0.18823529],\n",
       "        [0.16862745],\n",
       "        [0.34117647],\n",
       "        [0.65882353],\n",
       "        [0.52156863],\n",
       "        [0.0627451 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19215686],\n",
       "        [0.53333333],\n",
       "        [0.85882353],\n",
       "        [0.84705882],\n",
       "        [0.89411765],\n",
       "        [0.9254902 ],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [0.85098039],\n",
       "        [0.84313725],\n",
       "        [0.99607843],\n",
       "        [0.90588235],\n",
       "        [0.62745098],\n",
       "        [0.17647059],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.05490196],\n",
       "        [0.69019608],\n",
       "        [0.87058824],\n",
       "        [0.87843137],\n",
       "        [0.83137255],\n",
       "        [0.79607843],\n",
       "        [0.77647059],\n",
       "        [0.76862745],\n",
       "        [0.78431373],\n",
       "        [0.84313725],\n",
       "        [0.8       ],\n",
       "        [0.79215686],\n",
       "        [0.78823529],\n",
       "        [0.78823529],\n",
       "        [0.78823529],\n",
       "        [0.81960784],\n",
       "        [0.85490196],\n",
       "        [0.87843137],\n",
       "        [0.64313725],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.7372549 ],\n",
       "        [0.85882353],\n",
       "        [0.78431373],\n",
       "        [0.77647059],\n",
       "        [0.79215686],\n",
       "        [0.77647059],\n",
       "        [0.78039216],\n",
       "        [0.78039216],\n",
       "        [0.78823529],\n",
       "        [0.76862745],\n",
       "        [0.77647059],\n",
       "        [0.77647059],\n",
       "        [0.78431373],\n",
       "        [0.78431373],\n",
       "        [0.78431373],\n",
       "        [0.78431373],\n",
       "        [0.78823529],\n",
       "        [0.78431373],\n",
       "        [0.88235294],\n",
       "        [0.16078431],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.2       ],\n",
       "        [0.85882353],\n",
       "        [0.78039216],\n",
       "        [0.79607843],\n",
       "        [0.79607843],\n",
       "        [0.83137255],\n",
       "        [0.93333333],\n",
       "        [0.97254902],\n",
       "        [0.98039216],\n",
       "        [0.96078431],\n",
       "        [0.97647059],\n",
       "        [0.96470588],\n",
       "        [0.96862745],\n",
       "        [0.98823529],\n",
       "        [0.97254902],\n",
       "        [0.92156863],\n",
       "        [0.81176471],\n",
       "        [0.79607843],\n",
       "        [0.79607843],\n",
       "        [0.87058824],\n",
       "        [0.54901961],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.45490196],\n",
       "        [0.88627451],\n",
       "        [0.80784314],\n",
       "        [0.8       ],\n",
       "        [0.81176471],\n",
       "        [0.8       ],\n",
       "        [0.39607843],\n",
       "        [0.29411765],\n",
       "        [0.18431373],\n",
       "        [0.28627451],\n",
       "        [0.18823529],\n",
       "        [0.19607843],\n",
       "        [0.17647059],\n",
       "        [0.2       ],\n",
       "        [0.24705882],\n",
       "        [0.44313725],\n",
       "        [0.87058824],\n",
       "        [0.79215686],\n",
       "        [0.80784314],\n",
       "        [0.8627451 ],\n",
       "        [0.87843137],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.78431373],\n",
       "        [0.87058824],\n",
       "        [0.81960784],\n",
       "        [0.79607843],\n",
       "        [0.84313725],\n",
       "        [0.78431373],\n",
       "        [0.        ],\n",
       "        [0.2745098 ],\n",
       "        [0.38431373],\n",
       "        [0.        ],\n",
       "        [0.40392157],\n",
       "        [0.23137255],\n",
       "        [0.26666667],\n",
       "        [0.27843137],\n",
       "        [0.19215686],\n",
       "        [0.        ],\n",
       "        [0.85882353],\n",
       "        [0.80784314],\n",
       "        [0.83921569],\n",
       "        [0.82352941],\n",
       "        [0.98039216],\n",
       "        [0.14901961],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.96862745],\n",
       "        [0.85490196],\n",
       "        [0.83137255],\n",
       "        [0.82352941],\n",
       "        [0.84313725],\n",
       "        [0.83921569],\n",
       "        [0.        ],\n",
       "        [0.99607843],\n",
       "        [0.95294118],\n",
       "        [0.54509804],\n",
       "        [1.        ],\n",
       "        [0.68235294],\n",
       "        [0.98431373],\n",
       "        [1.        ],\n",
       "        [0.80392157],\n",
       "        [0.        ],\n",
       "        [0.84313725],\n",
       "        [0.85098039],\n",
       "        [0.83921569],\n",
       "        [0.81568627],\n",
       "        [0.8627451 ],\n",
       "        [0.37254902],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.17647059],\n",
       "        [0.88627451],\n",
       "        [0.83921569],\n",
       "        [0.83921569],\n",
       "        [0.84313725],\n",
       "        [0.87843137],\n",
       "        [0.80392157],\n",
       "        [0.        ],\n",
       "        [0.16470588],\n",
       "        [0.1372549 ],\n",
       "        [0.23529412],\n",
       "        [0.0627451 ],\n",
       "        [0.06666667],\n",
       "        [0.04705882],\n",
       "        [0.05098039],\n",
       "        [0.2745098 ],\n",
       "        [0.        ],\n",
       "        [0.74117647],\n",
       "        [0.84705882],\n",
       "        [0.83137255],\n",
       "        [0.80784314],\n",
       "        [0.83137255],\n",
       "        [0.61176471],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.64313725],\n",
       "        [0.92156863],\n",
       "        [0.83921569],\n",
       "        [0.82745098],\n",
       "        [0.8627451 ],\n",
       "        [0.84705882],\n",
       "        [0.78823529],\n",
       "        [0.20392157],\n",
       "        [0.27843137],\n",
       "        [0.34901961],\n",
       "        [0.36862745],\n",
       "        [0.3254902 ],\n",
       "        [0.30588235],\n",
       "        [0.2745098 ],\n",
       "        [0.29803922],\n",
       "        [0.36078431],\n",
       "        [0.34117647],\n",
       "        [0.80784314],\n",
       "        [0.81176471],\n",
       "        [0.87058824],\n",
       "        [0.83529412],\n",
       "        [0.85882353],\n",
       "        [0.81568627],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.41568627],\n",
       "        [0.73333333],\n",
       "        [0.8745098 ],\n",
       "        [0.92941176],\n",
       "        [0.97254902],\n",
       "        [0.82745098],\n",
       "        [0.77647059],\n",
       "        [0.98823529],\n",
       "        [0.98039216],\n",
       "        [0.97254902],\n",
       "        [0.96078431],\n",
       "        [0.97254902],\n",
       "        [0.98823529],\n",
       "        [0.99215686],\n",
       "        [0.98039216],\n",
       "        [0.98823529],\n",
       "        [0.9372549 ],\n",
       "        [0.78823529],\n",
       "        [0.83137255],\n",
       "        [0.88235294],\n",
       "        [0.84313725],\n",
       "        [0.75686275],\n",
       "        [0.44313725],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.06666667],\n",
       "        [0.21176471],\n",
       "        [0.62352941],\n",
       "        [0.87058824],\n",
       "        [0.75686275],\n",
       "        [0.81568627],\n",
       "        [0.75294118],\n",
       "        [0.77254902],\n",
       "        [0.78431373],\n",
       "        [0.78431373],\n",
       "        [0.78431373],\n",
       "        [0.78431373],\n",
       "        [0.78823529],\n",
       "        [0.79607843],\n",
       "        [0.76470588],\n",
       "        [0.82352941],\n",
       "        [0.64705882],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18431373],\n",
       "        [0.88235294],\n",
       "        [0.75294118],\n",
       "        [0.83921569],\n",
       "        [0.79607843],\n",
       "        [0.80784314],\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.80392157],\n",
       "        [0.80784314],\n",
       "        [0.8       ],\n",
       "        [0.83137255],\n",
       "        [0.77254902],\n",
       "        [0.85490196],\n",
       "        [0.41960784],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.02352941],\n",
       "        [0.        ],\n",
       "        [0.18039216],\n",
       "        [0.83137255],\n",
       "        [0.76470588],\n",
       "        [0.83137255],\n",
       "        [0.79215686],\n",
       "        [0.80784314],\n",
       "        [0.80392157],\n",
       "        [0.8       ],\n",
       "        [0.80392157],\n",
       "        [0.80784314],\n",
       "        [0.8       ],\n",
       "        [0.83137255],\n",
       "        [0.78431373],\n",
       "        [0.85490196],\n",
       "        [0.35686275],\n",
       "        [0.        ],\n",
       "        [0.01176471],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.04313725],\n",
       "        [0.77254902],\n",
       "        [0.78039216],\n",
       "        [0.80392157],\n",
       "        [0.79215686],\n",
       "        [0.80392157],\n",
       "        [0.80784314],\n",
       "        [0.8       ],\n",
       "        [0.80392157],\n",
       "        [0.81176471],\n",
       "        [0.8       ],\n",
       "        [0.80392157],\n",
       "        [0.80392157],\n",
       "        [0.85490196],\n",
       "        [0.30196078],\n",
       "        [0.        ],\n",
       "        [0.01960784],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.01176471],\n",
       "        [0.        ],\n",
       "        [0.00784314],\n",
       "        [0.74901961],\n",
       "        [0.77647059],\n",
       "        [0.78823529],\n",
       "        [0.80392157],\n",
       "        [0.80784314],\n",
       "        [0.80392157],\n",
       "        [0.80392157],\n",
       "        [0.80784314],\n",
       "        [0.81960784],\n",
       "        [0.80784314],\n",
       "        [0.78039216],\n",
       "        [0.81960784],\n",
       "        [0.85882353],\n",
       "        [0.29019608],\n",
       "        [0.        ],\n",
       "        [0.01960784],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.7372549 ],\n",
       "        [0.77254902],\n",
       "        [0.78431373],\n",
       "        [0.81176471],\n",
       "        [0.81176471],\n",
       "        [0.8       ],\n",
       "        [0.81176471],\n",
       "        [0.81176471],\n",
       "        [0.82352941],\n",
       "        [0.81568627],\n",
       "        [0.77647059],\n",
       "        [0.81176471],\n",
       "        [0.86666667],\n",
       "        [0.28235294],\n",
       "        [0.        ],\n",
       "        [0.01568627],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.84313725],\n",
       "        [0.77647059],\n",
       "        [0.79607843],\n",
       "        [0.80784314],\n",
       "        [0.81568627],\n",
       "        [0.80392157],\n",
       "        [0.81176471],\n",
       "        [0.81176471],\n",
       "        [0.82352941],\n",
       "        [0.81568627],\n",
       "        [0.78431373],\n",
       "        [0.79215686],\n",
       "        [0.87058824],\n",
       "        [0.29411765],\n",
       "        [0.        ],\n",
       "        [0.01568627],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.83137255],\n",
       "        [0.77647059],\n",
       "        [0.81960784],\n",
       "        [0.80784314],\n",
       "        [0.81960784],\n",
       "        [0.80784314],\n",
       "        [0.81568627],\n",
       "        [0.81176471],\n",
       "        [0.82745098],\n",
       "        [0.80784314],\n",
       "        [0.80392157],\n",
       "        [0.77647059],\n",
       "        [0.86666667],\n",
       "        [0.31372549],\n",
       "        [0.        ],\n",
       "        [0.01176471],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.8       ],\n",
       "        [0.78823529],\n",
       "        [0.80392157],\n",
       "        [0.81568627],\n",
       "        [0.81176471],\n",
       "        [0.80392157],\n",
       "        [0.82745098],\n",
       "        [0.80392157],\n",
       "        [0.82352941],\n",
       "        [0.82352941],\n",
       "        [0.81960784],\n",
       "        [0.76470588],\n",
       "        [0.86666667],\n",
       "        [0.37647059],\n",
       "        [0.        ],\n",
       "        [0.01176471],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.79215686],\n",
       "        [0.78823529],\n",
       "        [0.80392157],\n",
       "        [0.81960784],\n",
       "        [0.81176471],\n",
       "        [0.80392157],\n",
       "        [0.83529412],\n",
       "        [0.80784314],\n",
       "        [0.82352941],\n",
       "        [0.81960784],\n",
       "        [0.82352941],\n",
       "        [0.76078431],\n",
       "        [0.85098039],\n",
       "        [0.41176471],\n",
       "        [0.        ],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.80392157],\n",
       "        [0.81568627],\n",
       "        [0.81176471],\n",
       "        [0.80392157],\n",
       "        [0.84313725],\n",
       "        [0.81176471],\n",
       "        [0.82352941],\n",
       "        [0.81568627],\n",
       "        [0.82745098],\n",
       "        [0.75686275],\n",
       "        [0.83529412],\n",
       "        [0.45098039],\n",
       "        [0.        ],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.8       ],\n",
       "        [0.81176471],\n",
       "        [0.81176471],\n",
       "        [0.81568627],\n",
       "        [0.80784314],\n",
       "        [0.80784314],\n",
       "        [0.84313725],\n",
       "        [0.82352941],\n",
       "        [0.82352941],\n",
       "        [0.81176471],\n",
       "        [0.83137255],\n",
       "        [0.76470588],\n",
       "        [0.82352941],\n",
       "        [0.4627451 ],\n",
       "        [0.        ],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.77647059],\n",
       "        [0.81568627],\n",
       "        [0.81568627],\n",
       "        [0.81568627],\n",
       "        [0.8       ],\n",
       "        [0.81176471],\n",
       "        [0.83137255],\n",
       "        [0.83137255],\n",
       "        [0.82352941],\n",
       "        [0.81176471],\n",
       "        [0.82745098],\n",
       "        [0.76862745],\n",
       "        [0.81176471],\n",
       "        [0.4745098 ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.77647059],\n",
       "        [0.82352941],\n",
       "        [0.81176471],\n",
       "        [0.81568627],\n",
       "        [0.80784314],\n",
       "        [0.81960784],\n",
       "        [0.83529412],\n",
       "        [0.83137255],\n",
       "        [0.82745098],\n",
       "        [0.81176471],\n",
       "        [0.82352941],\n",
       "        [0.77254902],\n",
       "        [0.81176471],\n",
       "        [0.48627451],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.6745098 ],\n",
       "        [0.82352941],\n",
       "        [0.79607843],\n",
       "        [0.78823529],\n",
       "        [0.78039216],\n",
       "        [0.8       ],\n",
       "        [0.81176471],\n",
       "        [0.80392157],\n",
       "        [0.8       ],\n",
       "        [0.78823529],\n",
       "        [0.80392157],\n",
       "        [0.77254902],\n",
       "        [0.80784314],\n",
       "        [0.49803922],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.7372549 ],\n",
       "        [0.86666667],\n",
       "        [0.83921569],\n",
       "        [0.91764706],\n",
       "        [0.9254902 ],\n",
       "        [0.93333333],\n",
       "        [0.95686275],\n",
       "        [0.95686275],\n",
       "        [0.95686275],\n",
       "        [0.94117647],\n",
       "        [0.95294118],\n",
       "        [0.83921569],\n",
       "        [0.87843137],\n",
       "        [0.63529412],\n",
       "        [0.        ],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00392157],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.54509804],\n",
       "        [0.57254902],\n",
       "        [0.50980392],\n",
       "        [0.52941176],\n",
       "        [0.52941176],\n",
       "        [0.5372549 ],\n",
       "        [0.49019608],\n",
       "        [0.48627451],\n",
       "        [0.49019608],\n",
       "        [0.4745098 ],\n",
       "        [0.46666667],\n",
       "        [0.44705882],\n",
       "        [0.50980392],\n",
       "        [0.29803922],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 240,772\n",
      "Trainable params: 240,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5120 samples, validate on 1280 samples\n",
      "Epoch 1/50\n",
      "5120/5120 [==============================] - 5s 1ms/step - loss: 1.2217 - acc: 0.4488 - val_loss: 0.9672 - val_acc: 0.5687\n",
      "Epoch 2/50\n",
      "5120/5120 [==============================] - 5s 888us/step - loss: 0.9128 - acc: 0.6045 - val_loss: 0.7905 - val_acc: 0.6586\n",
      "Epoch 3/50\n",
      "5120/5120 [==============================] - 4s 877us/step - loss: 0.7822 - acc: 0.6514 - val_loss: 0.7256 - val_acc: 0.6945\n",
      "Epoch 4/50\n",
      "5120/5120 [==============================] - 5s 913us/step - loss: 0.7140 - acc: 0.6828 - val_loss: 0.6652 - val_acc: 0.7164\n",
      "Epoch 5/50\n",
      "5120/5120 [==============================] - 5s 882us/step - loss: 0.6821 - acc: 0.6941 - val_loss: 0.6570 - val_acc: 0.7242\n",
      "Epoch 6/50\n",
      "5120/5120 [==============================] - 5s 879us/step - loss: 0.6406 - acc: 0.7258 - val_loss: 0.6220 - val_acc: 0.7484\n",
      "Epoch 7/50\n",
      "5120/5120 [==============================] - 5s 889us/step - loss: 0.6050 - acc: 0.7443 - val_loss: 0.5805 - val_acc: 0.7672\n",
      "Epoch 8/50\n",
      "5120/5120 [==============================] - 5s 912us/step - loss: 0.5755 - acc: 0.7648 - val_loss: 0.5470 - val_acc: 0.7695\n",
      "Epoch 9/50\n",
      "5120/5120 [==============================] - 5s 961us/step - loss: 0.5465 - acc: 0.7721 - val_loss: 0.5227 - val_acc: 0.7977\n",
      "Epoch 10/50\n",
      "5120/5120 [==============================] - 5s 919us/step - loss: 0.5323 - acc: 0.7791 - val_loss: 0.5174 - val_acc: 0.8008\n",
      "Epoch 11/50\n",
      "5120/5120 [==============================] - 4s 878us/step - loss: 0.5065 - acc: 0.7961 - val_loss: 0.5124 - val_acc: 0.7992\n",
      "Epoch 12/50\n",
      "5120/5120 [==============================] - 5s 880us/step - loss: 0.4960 - acc: 0.8025 - val_loss: 0.5067 - val_acc: 0.7922\n",
      "Epoch 13/50\n",
      "5120/5120 [==============================] - 5s 890us/step - loss: 0.4836 - acc: 0.8109 - val_loss: 0.4789 - val_acc: 0.8227\n",
      "Epoch 14/50\n",
      "5120/5120 [==============================] - 5s 903us/step - loss: 0.4748 - acc: 0.8082 - val_loss: 0.4576 - val_acc: 0.8203\n",
      "Epoch 15/50\n",
      "5120/5120 [==============================] - 5s 901us/step - loss: 0.4537 - acc: 0.8174 - val_loss: 0.4565 - val_acc: 0.8219\n",
      "Epoch 16/50\n",
      "5120/5120 [==============================] - 4s 879us/step - loss: 0.4437 - acc: 0.8217 - val_loss: 0.4386 - val_acc: 0.8359\n",
      "Epoch 17/50\n",
      "5120/5120 [==============================] - 5s 945us/step - loss: 0.4410 - acc: 0.8213 - val_loss: 0.4377 - val_acc: 0.8313\n",
      "Epoch 18/50\n",
      "5120/5120 [==============================] - 5s 918us/step - loss: 0.4299 - acc: 0.8324 - val_loss: 0.4500 - val_acc: 0.8250\n",
      "Epoch 19/50\n",
      "5120/5120 [==============================] - 5s 890us/step - loss: 0.4121 - acc: 0.8346 - val_loss: 0.4255 - val_acc: 0.8352\n",
      "Epoch 20/50\n",
      "5120/5120 [==============================] - 5s 908us/step - loss: 0.4105 - acc: 0.8367 - val_loss: 0.4202 - val_acc: 0.8359\n",
      "Epoch 21/50\n",
      "5120/5120 [==============================] - 5s 883us/step - loss: 0.3974 - acc: 0.8406 - val_loss: 0.4138 - val_acc: 0.8430\n",
      "Epoch 22/50\n",
      "5120/5120 [==============================] - 5s 886us/step - loss: 0.3897 - acc: 0.8389 - val_loss: 0.4127 - val_acc: 0.8406\n",
      "Epoch 23/50\n",
      "5120/5120 [==============================] - 5s 962us/step - loss: 0.3812 - acc: 0.8461 - val_loss: 0.4209 - val_acc: 0.8406\n",
      "Epoch 24/50\n",
      "5120/5120 [==============================] - 5s 890us/step - loss: 0.3802 - acc: 0.8475 - val_loss: 0.4166 - val_acc: 0.8508\n",
      "Epoch 25/50\n",
      "5120/5120 [==============================] - 5s 893us/step - loss: 0.3748 - acc: 0.8457 - val_loss: 0.4040 - val_acc: 0.8477\n",
      "Epoch 26/50\n",
      "5120/5120 [==============================] - 5s 893us/step - loss: 0.3659 - acc: 0.8529 - val_loss: 0.3939 - val_acc: 0.8547\n",
      "Epoch 27/50\n",
      "5120/5120 [==============================] - 5s 894us/step - loss: 0.3579 - acc: 0.8594 - val_loss: 0.3969 - val_acc: 0.8523\n",
      "Epoch 28/50\n",
      "5120/5120 [==============================] - 5s 891us/step - loss: 0.3530 - acc: 0.8559 - val_loss: 0.3984 - val_acc: 0.8547\n",
      "Epoch 29/50\n",
      "5120/5120 [==============================] - 5s 897us/step - loss: 0.3540 - acc: 0.8555 - val_loss: 0.3933 - val_acc: 0.8508\n",
      "Epoch 30/50\n",
      "5120/5120 [==============================] - 5s 912us/step - loss: 0.3354 - acc: 0.8643 - val_loss: 0.3890 - val_acc: 0.8578\n",
      "Epoch 31/50\n",
      "5120/5120 [==============================] - 5s 888us/step - loss: 0.3414 - acc: 0.8594 - val_loss: 0.4047 - val_acc: 0.8516\n",
      "Epoch 32/50\n",
      "5120/5120 [==============================] - 5s 885us/step - loss: 0.3386 - acc: 0.8609 - val_loss: 0.3942 - val_acc: 0.8586\n",
      "Epoch 33/50\n",
      "5120/5120 [==============================] - 5s 893us/step - loss: 0.3293 - acc: 0.8664 - val_loss: 0.4100 - val_acc: 0.8555\n",
      "Epoch 34/50\n",
      "5120/5120 [==============================] - 5s 895us/step - loss: 0.3249 - acc: 0.8648 - val_loss: 0.3962 - val_acc: 0.8609\n",
      "Epoch 35/50\n",
      "5120/5120 [==============================] - 5s 881us/step - loss: 0.3170 - acc: 0.8691 - val_loss: 0.3985 - val_acc: 0.8539\n",
      "Epoch 36/50\n",
      "5120/5120 [==============================] - 5s 881us/step - loss: 0.3046 - acc: 0.8785 - val_loss: 0.3988 - val_acc: 0.8516\n",
      "Epoch 37/50\n",
      "5120/5120 [==============================] - 5s 882us/step - loss: 0.3065 - acc: 0.8799 - val_loss: 0.3880 - val_acc: 0.8617\n",
      "Epoch 38/50\n",
      "5120/5120 [==============================] - 5s 881us/step - loss: 0.3032 - acc: 0.8801 - val_loss: 0.3915 - val_acc: 0.8602\n",
      "Epoch 39/50\n",
      "5120/5120 [==============================] - 5s 892us/step - loss: 0.2988 - acc: 0.8828 - val_loss: 0.4047 - val_acc: 0.8539\n",
      "Epoch 40/50\n",
      "5120/5120 [==============================] - 5s 897us/step - loss: 0.2997 - acc: 0.8805 - val_loss: 0.3947 - val_acc: 0.8664\n",
      "Epoch 41/50\n",
      "5120/5120 [==============================] - 5s 883us/step - loss: 0.2829 - acc: 0.8871 - val_loss: 0.3865 - val_acc: 0.8656\n",
      "Epoch 42/50\n",
      "5120/5120 [==============================] - 4s 873us/step - loss: 0.2903 - acc: 0.8830 - val_loss: 0.3774 - val_acc: 0.8594\n",
      "Epoch 43/50\n",
      "5120/5120 [==============================] - 4s 875us/step - loss: 0.2884 - acc: 0.8869 - val_loss: 0.3835 - val_acc: 0.8664\n",
      "Epoch 44/50\n",
      "5120/5120 [==============================] - 5s 879us/step - loss: 0.2842 - acc: 0.8830 - val_loss: 0.3926 - val_acc: 0.8641\n",
      "Epoch 45/50\n",
      "5120/5120 [==============================] - 5s 998us/step - loss: 0.2683 - acc: 0.8916 - val_loss: 0.3947 - val_acc: 0.8641\n",
      "Epoch 46/50\n",
      "5120/5120 [==============================] - 6s 1ms/step - loss: 0.2689 - acc: 0.8902 - val_loss: 0.3835 - val_acc: 0.8703\n",
      "Epoch 47/50\n",
      "5120/5120 [==============================] - 6s 1ms/step - loss: 0.2719 - acc: 0.8881 - val_loss: 0.4023 - val_acc: 0.8602\n",
      "Epoch 48/50\n",
      "5120/5120 [==============================] - 5s 940us/step - loss: 0.2678 - acc: 0.8873 - val_loss: 0.3935 - val_acc: 0.8609\n",
      "Epoch 49/50\n",
      "5120/5120 [==============================] - 5s 908us/step - loss: 0.2544 - acc: 0.8961 - val_loss: 0.3916 - val_acc: 0.8719\n",
      "Epoch 50/50\n",
      "5120/5120 [==============================] - 5s 899us/step - loss: 0.2604 - acc: 0.8957 - val_loss: 0.3775 - val_acc: 0.8703\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.37477672919631005\n",
      "Test accuracy: 0.866875\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True,)\n",
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='./weights.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 7s 362ms/step - loss: 1.1676 - acc: 0.4561 - val_loss: 0.8676 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86762, saving model to ./weights.hdf5\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 6s 312ms/step - loss: 0.8460 - acc: 0.6246 - val_loss: 0.7161 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.86762 to 0.71612, saving model to ./weights.hdf5\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 5s 238ms/step - loss: 0.7347 - acc: 0.6693 - val_loss: 0.6875 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71612 to 0.68746, saving model to ./weights.hdf5\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 5s 233ms/step - loss: 0.6709 - acc: 0.7002 - val_loss: 0.6521 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68746 to 0.65210, saving model to ./weights.hdf5\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 5s 228ms/step - loss: 0.6429 - acc: 0.7111 - val_loss: 0.5924 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65210 to 0.59241, saving model to ./weights.hdf5\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 5s 247ms/step - loss: 0.5877 - acc: 0.7568 - val_loss: 0.5636 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.59241 to 0.56358, saving model to ./weights.hdf5\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 5s 232ms/step - loss: 0.5606 - acc: 0.7678 - val_loss: 0.5349 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56358 to 0.53493, saving model to ./weights.hdf5\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 5s 229ms/step - loss: 0.5470 - acc: 0.7723 - val_loss: 0.5166 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.53493 to 0.51665, saving model to ./weights.hdf5\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 5s 242ms/step - loss: 0.5156 - acc: 0.7912 - val_loss: 0.4964 - val_acc: 0.8031\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51665 to 0.49645, saving model to ./weights.hdf5\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 5s 234ms/step - loss: 0.5000 - acc: 0.7908 - val_loss: 0.5123 - val_acc: 0.7906\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49645\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 5s 236ms/step - loss: 0.4912 - acc: 0.7977 - val_loss: 0.4750 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49645 to 0.47496, saving model to ./weights.hdf5\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 5s 248ms/step - loss: 0.4746 - acc: 0.8107 - val_loss: 0.4580 - val_acc: 0.8289\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47496 to 0.45804, saving model to ./weights.hdf5\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 5s 236ms/step - loss: 0.4656 - acc: 0.8115 - val_loss: 0.4586 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.45804\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 5s 249ms/step - loss: 0.4585 - acc: 0.8115 - val_loss: 0.4544 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.45804 to 0.45445, saving model to ./weights.hdf5\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 5s 239ms/step - loss: 0.4409 - acc: 0.8236 - val_loss: 0.4426 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.45445 to 0.44261, saving model to ./weights.hdf5\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 5s 226ms/step - loss: 0.4358 - acc: 0.8219 - val_loss: 0.4299 - val_acc: 0.8313\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.44261 to 0.42994, saving model to ./weights.hdf5\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 5s 231ms/step - loss: 0.4175 - acc: 0.8299 - val_loss: 0.4355 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.42994\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 5s 229ms/step - loss: 0.4209 - acc: 0.8285 - val_loss: 0.4206 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.42994 to 0.42059, saving model to ./weights.hdf5\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 5s 241ms/step - loss: 0.4050 - acc: 0.8369 - val_loss: 0.4179 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.42059 to 0.41790, saving model to ./weights.hdf5\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 5s 262ms/step - loss: 0.4066 - acc: 0.8354 - val_loss: 0.4168 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.41790 to 0.41677, saving model to ./weights.hdf5\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.3874 - acc: 0.8432 - val_loss: 0.4220 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.41677\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 5s 238ms/step - loss: 0.3806 - acc: 0.8432 - val_loss: 0.4126 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.41677 to 0.41258, saving model to ./weights.hdf5\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 6s 278ms/step - loss: 0.3786 - acc: 0.8480 - val_loss: 0.4322 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41258\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 5s 240ms/step - loss: 0.3749 - acc: 0.8492 - val_loss: 0.4121 - val_acc: 0.8352\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.41258 to 0.41214, saving model to ./weights.hdf5\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 7s 346ms/step - loss: 0.3698 - acc: 0.8475 - val_loss: 0.3932 - val_acc: 0.8578\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.41214 to 0.39320, saving model to ./weights.hdf5\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 6s 295ms/step - loss: 0.3645 - acc: 0.8562 - val_loss: 0.3976 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.39320\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 5s 266ms/step - loss: 0.3493 - acc: 0.8596 - val_loss: 0.3822 - val_acc: 0.8578\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.39320 to 0.38216, saving model to ./weights.hdf5\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 4s 218ms/step - loss: 0.3517 - acc: 0.8598 - val_loss: 0.3936 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.38216\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 4s 221ms/step - loss: 0.3452 - acc: 0.8621 - val_loss: 0.3846 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.38216\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 0.3452 - acc: 0.8600 - val_loss: 0.3857 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.38216\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 4s 199ms/step - loss: 0.3367 - acc: 0.8615 - val_loss: 0.3750 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.38216 to 0.37504, saving model to ./weights.hdf5\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.3344 - acc: 0.8637 - val_loss: 0.3826 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.37504\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 5s 226ms/step - loss: 0.3310 - acc: 0.8682 - val_loss: 0.3763 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.37504\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 4s 207ms/step - loss: 0.3250 - acc: 0.8699 - val_loss: 0.3728 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.37504 to 0.37283, saving model to ./weights.hdf5\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 4s 207ms/step - loss: 0.3273 - acc: 0.8684 - val_loss: 0.3699 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.37283 to 0.36989, saving model to ./weights.hdf5\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 4s 199ms/step - loss: 0.3166 - acc: 0.8723 - val_loss: 0.3685 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.36989 to 0.36852, saving model to ./weights.hdf5\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 4s 217ms/step - loss: 0.3072 - acc: 0.8748 - val_loss: 0.3656 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.36852 to 0.36564, saving model to ./weights.hdf5\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 4s 198ms/step - loss: 0.3082 - acc: 0.8754 - val_loss: 0.3534 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.36564 to 0.35341, saving model to ./weights.hdf5\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 5s 248ms/step - loss: 0.3032 - acc: 0.8754 - val_loss: 0.3556 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.35341\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 266ms/step - loss: 0.3004 - acc: 0.8850 - val_loss: 0.3538 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.35341\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 5s 255ms/step - loss: 0.2908 - acc: 0.8832 - val_loss: 0.3556 - val_acc: 0.8695\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.35341\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.2911 - acc: 0.8863 - val_loss: 0.3591 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.35341\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 4s 196ms/step - loss: 0.2973 - acc: 0.8799 - val_loss: 0.3620 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.35341\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 4s 197ms/step - loss: 0.2924 - acc: 0.8832 - val_loss: 0.3736 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.35341\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 4s 197ms/step - loss: 0.2738 - acc: 0.8893 - val_loss: 0.3569 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.35341\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 4s 197ms/step - loss: 0.2719 - acc: 0.8869 - val_loss: 0.3600 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.35341\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 4s 196ms/step - loss: 0.2763 - acc: 0.8906 - val_loss: 0.3615 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.35341\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 5s 241ms/step - loss: 0.2821 - acc: 0.8871 - val_loss: 0.3734 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.35341\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 4s 197ms/step - loss: 0.2693 - acc: 0.8939 - val_loss: 0.3545 - val_acc: 0.8773\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.35341\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.2685 - acc: 0.8916 - val_loss: 0.3541 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.35341\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_datagen.flow(X_train,y_train,batch_size),\n",
    "              epochs=epochs,\n",
    "              steps_per_epoch = len(X_train)//batch_size,  \n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val),\n",
    "                             callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.357341883033514\n",
      "Test accuracy: 0.86375\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
